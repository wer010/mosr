import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.nn import functional as F
import matplotlib.pyplot as plt

print("=" * 80)
print("L2L (Learning to Learn) vs MAML (Model-Agnostic Meta-Learning)")
print("=" * 80)


# ==================== L2L å®ç° ====================
class L2LOptimizer(nn.Module):
    """
    L2L: å­¦ä¹ ä¼˜åŒ–å™¨æœ¬èº«
    ç›®æ ‡ï¼šæ›¿ä»£ä¼ ç»Ÿä¼˜åŒ–å™¨ (SGD, Adamç­‰)
    """

    def __init__(self, input_dim=1, hidden_dim=64):
        super(L2LOptimizer, self).__init__()

        print("\nğŸ”µ L2L ä¼˜åŒ–å™¨åˆå§‹åŒ–:")
        print("   - ç›®æ ‡: å­¦ä¹ å¦‚ä½•ä¼˜åŒ– (å­¦ä¹ ä¼˜åŒ–å™¨)")
        print("   - è¾“å…¥: [æ¢¯åº¦, å‚æ•°, å†å²ä¿¡æ¯]")
        print("   - è¾“å‡º: å‚æ•°æ›´æ–°é‡")

        # å¤„ç†è¾“å…¥ç‰¹å¾ [gradient, parameter, momentumç­‰]
        self.input_processor = nn.Sequential(
            nn.Linear(input_dim * 3, hidden_dim),  # grad, param, momentum
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # LSTMç»´æŠ¤ä¼˜åŒ–å†å²
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)

        # è¾“å‡ºæ›´æ–°é‡
        self.output_layer = nn.Linear(hidden_dim, input_dim)

        # ä¼˜åŒ–å™¨çŠ¶æ€
        self.hidden_state = None
        self.momentum = None

    def forward(self, gradients, parameters):
        """
        L2Lçš„æ ¸å¿ƒï¼šå­¦ä¹ å¦‚ä½•æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°
        """
        batch_size = gradients.size(0)

        # åˆå§‹åŒ–åŠ¨é‡
        if self.momentum is None:
            self.momentum = torch.zeros_like(parameters)

        # æ›´æ–°åŠ¨é‡
        self.momentum = 0.9 * self.momentum + gradients

        # æ„å»ºè¾“å…¥ç‰¹å¾
        input_features = torch.cat([gradients, parameters, self.momentum], dim=-1)

        # å¤„ç†è¾“å…¥
        processed = self.input_processor(input_features)

        # åˆå§‹åŒ–LSTMçŠ¶æ€
        if self.hidden_state is None:
            self.hidden_state = (
                torch.zeros(1, batch_size, 64),
                torch.zeros(1, batch_size, 64)
            )

        # LSTMå‰å‘ä¼ æ’­
        # Disable CuDNN for higher-order gradients (meta-learning compatibility)
        with torch.backends.cudnn.flags(enabled=False):
            lstm_out, self.hidden_state = self.lstm(processed.unsqueeze(1), self.hidden_state)

        # è¾“å‡ºæ›´æ–°é‡
        update = self.output_layer(lstm_out.squeeze(1))

        return update

    def reset_state(self):
        """é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€"""
        self.hidden_state = None
        self.momentum = None


# ==================== MAML å®ç° ====================
class SimpleModel(nn.Module):
    """
    MAMLä¸­çš„åŸºç¡€æ¨¡å‹
    """

    def __init__(self, input_dim=1, hidden_dim=64, output_dim=1):
        super(SimpleModel, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        return self.net(x)


class MAML:
    """
    MAML: å­¦ä¹ æ¨¡å‹çš„åˆå§‹åŒ–
    ç›®æ ‡ï¼šæ‰¾åˆ°å¥½çš„åˆå§‹å‚æ•°ï¼Œä½¿å¾—èƒ½å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
    """

    def __init__(self, model, inner_lr=0.01, meta_lr=0.001):
        self.model = model
        self.inner_lr = inner_lr
        self.meta_optimizer = optim.Adam(model.parameters(), lr=meta_lr)

        print("\nğŸ”´ MAML åˆå§‹åŒ–:")
        print("   - ç›®æ ‡: å­¦ä¹ æ¨¡å‹åˆå§‹åŒ– (å­¦ä¹ æ¨¡å‹å‚æ•°)")
        print("   - å†…å¾ªç¯: å¿«é€Ÿé€‚åº”å•ä¸ªä»»åŠ¡")
        print("   - å¤–å¾ªç¯: ä¼˜åŒ–åˆå§‹åŒ–å‚æ•°")
        print(f"   - å†…å¾ªç¯å­¦ä¹ ç‡: {inner_lr}")
        print(f"   - å…ƒå­¦ä¹ ç‡: {meta_lr}")

    def inner_loop(self, task_data, task_labels, num_steps=5):
        """
        MAMLå†…å¾ªç¯ï¼šåœ¨å•ä¸ªä»»åŠ¡ä¸Šå¿«é€Ÿé€‚åº”
        """
        # å¤åˆ¶å½“å‰å‚æ•°
        fast_weights = {}
        for name, param in self.model.named_parameters():
            fast_weights[name] = param.clone()

        # å†…å¾ªç¯æ›´æ–°
        for step in range(num_steps):
            # å‰å‘ä¼ æ’­
            logits = self.functional_forward(task_data, fast_weights)
            loss = F.mse_loss(logits, task_labels)

            # è®¡ç®—æ¢¯åº¦
            grads = torch.autograd.grad(loss, fast_weights.values(),
                                        create_graph=True, retain_graph=True)

            # æ›´æ–°fast_weights
            for (name, param), grad in zip(fast_weights.items(), grads):
                fast_weights[name] = param - self.inner_lr * grad

        return fast_weights

    def functional_forward(self, x, weights):
        """ä½¿ç”¨ç»™å®šæƒé‡çš„å‡½æ•°å¼å‰å‘ä¼ æ’­"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…ä¸­éœ€è¦æ›´å¤æ‚çš„æƒé‡ç®¡ç†
        x = F.linear(x, weights['net.0.weight'], weights['net.0.bias'])
        x = F.relu(x)
        x = F.linear(x, weights['net.2.weight'], weights['net.2.bias'])
        x = F.relu(x)
        x = F.linear(x, weights['net.4.weight'], weights['net.4.bias'])
        return x

    def meta_update(self, task_batch):
        """
        MAMLå¤–å¾ªç¯ï¼šå…ƒæ›´æ–°
        """
        meta_loss = 0

        for task_data, task_labels in task_batch:
            # å†…å¾ªç¯é€‚åº”
            fast_weights = self.inner_loop(task_data, task_labels)

            # åœ¨adapted weightsä¸Šè®¡ç®—æŸå¤±
            adapted_logits = self.functional_forward(task_data, fast_weights)
            task_loss = F.mse_loss(adapted_logits, task_labels)
            meta_loss += task_loss

        # å…ƒæ¢¯åº¦æ›´æ–°
        meta_loss = meta_loss / len(task_batch)
        self.meta_optimizer.zero_grad()
        meta_loss.backward()
        self.meta_optimizer.step()

        return meta_loss.item()


# ==================== å¯¹æ¯”å®éªŒ ====================
def compare_l2l_vs_maml():
    """
    è¯¦ç»†å¯¹æ¯”L2Lå’ŒMAMLçš„å·®å¼‚
    """
    print("\n" + "=" * 60)
    print("è¯¦ç»†å¯¹æ¯”å®éªŒ")
    print("=" * 60)

    # åˆ›å»ºç®€å•çš„ä¼˜åŒ–é—®é¢˜
    def create_quadratic_task(a, b, c):
        """åˆ›å»ºäºŒæ¬¡å‡½æ•°ä»»åŠ¡ f(x) = axÂ² + bx + c"""

        def objective(x):
            return a * x ** 2 + b * x + c

        def gradient(x):
            return 2 * a * x + b

        return objective, gradient

    # ==================== L2L å®éªŒ ====================
    print("\nğŸ”µ L2L å®éªŒ:")
    print("åœºæ™¯: å­¦ä¹ å¦‚ä½•ä¼˜åŒ–ä¸åŒçš„äºŒæ¬¡å‡½æ•°")

    l2l_optimizer = L2LOptimizer(input_dim=1, hidden_dim=32)
    l2l_meta_optimizer = optim.Adam(l2l_optimizer.parameters(), lr=0.001)

    # L2Lå…ƒè®­ç»ƒ
    l2l_losses = []
    for epoch in range(100):
        epoch_loss = 0

        # é‡‡æ ·å¤šä¸ªä»»åŠ¡
        for _ in range(10):
            # éšæœºç”Ÿæˆä»»åŠ¡å‚æ•°
            a, b, c = np.random.uniform(-2, 2, 3)
            objective, gradient = create_quadratic_task(a, b, c)

            # éšæœºåˆå§‹åŒ–å‚æ•°
            x = torch.tensor([np.random.uniform(-3, 3)], requires_grad=True)
            l2l_optimizer.reset_state()

            # ä½¿ç”¨L2Lä¼˜åŒ–å™¨è¿›è¡Œä¼˜åŒ–
            for step in range(10):
                loss = objective(x)
                loss.backward()

                with torch.no_grad():
                    # ä½¿ç”¨L2Lä¼˜åŒ–å™¨è®¡ç®—æ›´æ–°
                    update = l2l_optimizer(x.grad.unsqueeze(0), x.unsqueeze(0))
                    x = x - update.squeeze()
                    x.requires_grad_(True)

            # æœ€ç»ˆæŸå¤±
            final_loss = objective(x)
            epoch_loss += final_loss.item()

        # å…ƒæ›´æ–°
        avg_loss = epoch_loss / 10
        l2l_meta_optimizer.zero_grad()
        torch.tensor(avg_loss, requires_grad=True).backward()
        l2l_meta_optimizer.step()

        l2l_losses.append(avg_loss)

        if epoch % 20 == 0:
            print(f"   Epoch {epoch}: å¹³å‡æŸå¤± = {avg_loss:.4f}")

    # ==================== MAML å®éªŒ ====================
    print("\nğŸ”´ MAML å®éªŒ:")
    print("åœºæ™¯: å­¦ä¹ å‡½æ•°æ‹Ÿåˆçš„åˆå§‹åŒ–")

    model = SimpleModel(input_dim=1, hidden_dim=32, output_dim=1)
    maml = MAML(model, inner_lr=0.01, meta_lr=0.001)

    # MAMLå…ƒè®­ç»ƒ
    maml_losses = []
    for epoch in range(100):
        # åˆ›å»ºä»»åŠ¡æ‰¹æ¬¡
        task_batch = []
        for _ in range(5):
            # éšæœºç”Ÿæˆä»»åŠ¡
            a, b, c = np.random.uniform(-2, 2, 3)

            # ç”Ÿæˆä»»åŠ¡æ•°æ®
            x_task = torch.randn(10, 1)
            y_task = a * x_task ** 2 + b * x_task + c

            task_batch.append((x_task, y_task))

        # å…ƒæ›´æ–°
        meta_loss = maml.meta_update(task_batch)
        maml_losses.append(meta_loss)

        if epoch % 20 == 0:
            print(f"   Epoch {epoch}: å…ƒæŸå¤± = {meta_loss:.4f}")

    # ==================== æµ‹è¯•å¯¹æ¯” ====================
    print("\n" + "=" * 60)
    print("æµ‹è¯•é˜¶æ®µå¯¹æ¯”")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    test_a, test_b, test_c = 1.5, -1.0, 0.5
    test_objective, test_gradient = create_quadratic_task(test_a, test_b, test_c)

    # L2Læµ‹è¯•
    print("\nğŸ”µ L2L æµ‹è¯•ç»“æœ:")
    x_l2l = torch.tensor([2.0], requires_grad=True)
    l2l_optimizer.reset_state()

    print(f"   åˆå§‹: x = {x_l2l.item():.3f}, loss = {test_objective(x_l2l).item():.4f}")

    for step in range(10):
        loss = test_objective(x_l2l)
        loss.backward()

        with torch.no_grad():
            update = l2l_optimizer(x_l2l.grad.unsqueeze(0), x_l2l.unsqueeze(0))
            x_l2l = x_l2l - update.squeeze()
            x_l2l.requires_grad_(True)

    print(f"   æœ€ç»ˆ: x = {x_l2l.item():.3f}, loss = {test_objective(x_l2l).item():.4f}")

    # MAMLæµ‹è¯•
    print("\nğŸ”´ MAML æµ‹è¯•ç»“æœ:")
    x_test = torch.randn(5, 1)
    y_test = test_a * x_test ** 2 + test_b * x_test + test_c

    # å¿«é€Ÿé€‚åº”
    fast_weights = maml.inner_loop(x_test, y_test, num_steps=10)

    # æµ‹è¯•é€‚åº”åçš„æ€§èƒ½
    test_pred = maml.functional_forward(x_test, fast_weights)
    test_loss = F.mse_loss(test_pred, y_test)

    print(f"   å¿«é€Ÿé€‚åº”åæµ‹è¯•æŸå¤±: {test_loss.item():.4f}")

    return l2l_losses, maml_losses


# ==================== å…³é”®åŒºåˆ«æ€»ç»“ ====================
def summarize_differences():
    """
    æ€»ç»“L2Lå’ŒMAMLçš„å…³é”®åŒºåˆ«
    """
    print("\n" + "=" * 80)
    print("ğŸ¯ L2L vs MAML å…³é”®åŒºåˆ«æ€»ç»“")
    print("=" * 80)

    print("\nğŸ“Š å¯¹æ¯”ç»´åº¦:")
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚   å¯¹æ¯”ç»´åº¦      â”‚        L2L          â”‚       MAML          â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ å­¦ä¹ ç›®æ ‡        â”‚ å­¦ä¹ ä¼˜åŒ–å™¨          â”‚ å­¦ä¹ æ¨¡å‹åˆå§‹åŒ–      â”‚")
    print("â”‚ åº”ç”¨åœºæ™¯        â”‚ ä¼˜åŒ–é—®é¢˜            â”‚ æ¨¡å‹é€‚åº”            â”‚")
    print("â”‚ è¾“å…¥            â”‚ æ¢¯åº¦+å‚æ•°+å†å²      â”‚ ä»»åŠ¡æ•°æ®            â”‚")
    print("â”‚ è¾“å‡º            â”‚ å‚æ•°æ›´æ–°é‡          â”‚ æ¨¡å‹é¢„æµ‹            â”‚")
    print("â”‚ æ›¿ä»£å¯¹è±¡        â”‚ SGD/Adamç­‰ä¼˜åŒ–å™¨    â”‚ éšæœºåˆå§‹åŒ–          â”‚")
    print("â”‚ å†…å¾ªç¯          â”‚ ä¼˜åŒ–æ­¥éª¤            â”‚ æ¢¯åº¦ä¸‹é™            â”‚")
    print("â”‚ å¤–å¾ªç¯          â”‚ ä¼˜åŒ–å™¨å‚æ•°æ›´æ–°      â”‚ åˆå§‹åŒ–å‚æ•°æ›´æ–°      â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    print("\nğŸ” å…·ä½“å·®å¼‚:")

    print("\n1ï¸âƒ£ å­¦ä¹ å†…å®¹ä¸åŒ:")
    print("   ğŸ”µ L2L: å­¦ä¹  'å¦‚ä½•ä¼˜åŒ–' - å³ä¼˜åŒ–ç®—æ³•æœ¬èº«")
    print("   ğŸ”´ MAML: å­¦ä¹  'å¥½çš„èµ·ç‚¹' - å³å¥½çš„åˆå§‹åŒ–å‚æ•°")

    print("\n2ï¸âƒ£ åº”ç”¨æ–¹å¼ä¸åŒ:")
    print("   ğŸ”µ L2L: æ›¿æ¢ä¼ ç»Ÿä¼˜åŒ–å™¨ (SGD â†’ L2L)")
    print("   ğŸ”´ MAML: æä¾›å¥½çš„åˆå§‹åŒ– (éšæœºåˆå§‹åŒ– â†’ MAMLåˆå§‹åŒ–)")

    print("\n3ï¸âƒ£ é€‚ç”¨é—®é¢˜ä¸åŒ:")
    print("   ğŸ”µ L2L: æ›´é€‚åˆä½ çš„å‚æ•°ä¼˜åŒ–é—®é¢˜")
    print("        - å­¦ä¹ theta, beta, phiçš„ä¼˜åŒ–æ¨¡å¼")
    print("        - å¤„ç†å‚æ•°é—´çš„è€¦åˆå…³ç³»")
    print("        - è‡ªé€‚åº”è°ƒæ•´ä¼˜åŒ–ç­–ç•¥")

    print("\n   ğŸ”´ MAML: æ›´é€‚åˆæ¨¡å‹é€‚åº”é—®é¢˜")
    print("        - å›¾åƒåˆ†ç±»çš„few-shotå­¦ä¹ ")
    print("        - æ–°ç”¨æˆ·æ¨èç³»ç»Ÿé€‚åº”")
    print("        - å¤šä»»åŠ¡å­¦ä¹ ")

    print("\n4ï¸âƒ£ ç»„åˆä½¿ç”¨:")
    print("   ğŸ’¡ å®é™…ä¸Šå¯ä»¥ç»„åˆä½¿ç”¨:")
    print("      MAMLæä¾›å¥½çš„åˆå§‹åŒ– + L2Læä¾›å¥½çš„ä¼˜åŒ–å™¨")
    print("      = æœ€å¼ºçš„å…ƒå­¦ä¹ ç³»ç»Ÿ")


# ==================== é’ˆå¯¹ä½ çš„ç‰©ç†é—®é¢˜çš„å»ºè®® ====================
def recommendation_for_physics_problem():
    """
    é’ˆå¯¹ç‰©ç†å‚æ•°ä¼˜åŒ–é—®é¢˜çš„å…·ä½“å»ºè®®
    """
    print("\n" + "=" * 80)
    print("ğŸ¯ é’ˆå¯¹ä½ çš„ç‰©ç†é—®é¢˜çš„å»ºè®®")
    print("=" * 80)

    print("\nä½ çš„é—®é¢˜ç‰¹ç‚¹:")
    print("âœ“ ç›®æ ‡å‡½æ•°: å·²è®¾è®¡å¥½çš„ç‰©ç†ç›®æ ‡å‡½æ•°")
    print("âœ“ å‚æ•°: theta, beta, phi (æœ‰ç‰©ç†æ„ä¹‰)")
    print("âœ“ éœ€æ±‚: æ›´å¥½çš„å‚æ•°ä¼˜åŒ–æ–¹æ³•")

    print("\nğŸ’¡ æ¨èæ–¹æ¡ˆ:")
    print("1ï¸âƒ£ ä¸»è¦ä½¿ç”¨ L2L:")
    print("   - å­¦ä¹ å¦‚ä½•ä¼˜åŒ–ä½ çš„ç‰©ç†å‚æ•°")
    print("   - å¤„ç†å‚æ•°é—´çš„ç‰©ç†è€¦åˆå…³ç³»")
    print("   - è‡ªé€‚åº”ä¸åŒçš„ç‰©ç†æ¡ä»¶")

    print("\n2ï¸âƒ£ å¯é€‰çš„ç»„åˆæ–¹æ¡ˆ:")
    print("   - L2L + ç‰©ç†çº¦æŸ: åœ¨L2Lä¸­åµŒå…¥ç‰©ç†çº¦æŸ")
    print("   - L2L + è´å¶æ–¯ä¼˜åŒ–: L2Lå¤„ç†å±€éƒ¨ä¼˜åŒ–ï¼Œè´å¶æ–¯å¤„ç†å…¨å±€æœç´¢")
    print("   - å¤šå°ºåº¦L2L: ä¸åŒå°ºåº¦çš„ç‰©ç†å‚æ•°ä½¿ç”¨ä¸åŒçš„L2L")

    print("\n3ï¸âƒ£ å®ç°å»ºè®®:")
    print("   - æ”¶é›†å†å²ä¼˜åŒ–æ•°æ®è¿›è¡Œå…ƒè®­ç»ƒ")
    print("   - è®¾è®¡ä»»åŠ¡åˆ†å¸ƒæ¥æ¨¡æ‹Ÿä¸åŒç‰©ç†æ¡ä»¶")
    print("   - åœ¨L2Lä¸­èå…¥ç‰©ç†å…ˆéªŒçŸ¥è¯†")

    print("\nğŸš€ é¢„æœŸæ•ˆæœ:")
    print("   - æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦")
    print("   - æ›´å¥½çš„å…¨å±€æœ€ä¼˜è§£")
    print("   - è‡ªé€‚åº”ä¸åŒç‰©ç†æ¡ä»¶")
    print("   - å‡å°‘æ‰‹åŠ¨è°ƒå‚å·¥ä½œ")


if __name__ == "__main__":
    # è¿è¡Œå¯¹æ¯”å®éªŒ
    l2l_losses, maml_losses = compare_l2l_vs_maml()

    # æ€»ç»“åŒºåˆ«
    summarize_differences()

    # é’ˆå¯¹ç‰©ç†é—®é¢˜çš„å»ºè®®
    recommendation_for_physics_problem()

    print("\n" + "=" * 80)
    print("ğŸ‰ æ€»ç»“: å¯¹äºä½ çš„ç‰©ç†å‚æ•°ä¼˜åŒ–é—®é¢˜ï¼ŒL2Læ˜¯æ›´å¥½çš„é€‰æ‹©ï¼")
    print("=" * 80)